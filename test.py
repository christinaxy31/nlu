{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb83897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load truthfulqa.py\n",
    "\"\"\"\n",
    "Code for HW 3. The code in this file can be used as either a script or a\n",
    "module. You will implement the MultipleChoicePipeline class in Problem\n",
    "2. In Problem 3, you will test LLMs on TruthfulQA by running this file\n",
    "as a script on HPC.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import Pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\"\"\" Helper functions \"\"\"\n",
    "\n",
    "\n",
    "def print_delay(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Print statements often interrupt tqdm progress bars, since the\n",
    "    latter are set to stderr rather than stdout. This is a wrapper\n",
    "    around Python's print function that deals with such timing issues by\n",
    "    waiting for 0.1 seconds before and after printing. This function\n",
    "    should be used whenever you want to print something right before\n",
    "    starting a tqdm loop.\n",
    "    \"\"\"\n",
    "    time.sleep(.1)\n",
    "    print(*args, **kwargs)\n",
    "    time.sleep(.1)\n",
    "\n",
    "\n",
    "def _sanitize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces all punctuation in a text by '-' for the purpose of\n",
    "    creating filenames.\n",
    "    \"\"\"\n",
    "    return re.sub('[^0-9a-zA-Z]+', '-', text)\n",
    "\n",
    "\n",
    "\"\"\" Code to evaluate a language model on TruthfulQA \"\"\"\n",
    "\n",
    "# Data structure for storing model outputs\n",
    "Output = namedtuple(\"Output\", \"loss prediction\")\n",
    "\n",
    "# The accuracy metric from ðŸ¤— Evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "class MultipleChoicePipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    This is a Hugging Face pipeline for doing multiple-choice question\n",
    "    answering with large language models (LLMs). It is designed to be\n",
    "    compatible with the EleutherAI/truthful_qa_mc dataset on the Hugging\n",
    "    Face Hub. You will complete the implementation of this pipeline in\n",
    "    Problem 2.\n",
    "\n",
    "    This pipeline takes a batch of questions as input, where each\n",
    "    question is accompanied by some number of answer choices. The LLM\n",
    "    chooses an answer for each question by concatenating each answer\n",
    "    choice with the prompt and question, and choosing the answer choice\n",
    "    that minimizes total cross-entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str, num_choices: int = 4):\n",
    "        \"\"\"\n",
    "        Before starting your implementation, please take a look at this\n",
    "        function and the class definition in order to see what instance\n",
    "        variables and methods are available to you.\n",
    "\n",
    "        :param model: The Hugging Face path to a pre-trained LLM\n",
    "        :param num_choices: The number of answer choices per question\n",
    "        \"\"\"\n",
    "        self.num_choices = num_choices\n",
    "\n",
    "        # Load the LLM and tokenizer\n",
    "        lm = AutoModelForCausalLM.from_pretrained(model)\n",
    "        lm.eval()\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        if tokenizer.pad_token is None:  # GPT-2 doesn't have a pad token\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Use GPU if it's available\n",
    "        device = 0 if torch.cuda.is_available() else None\n",
    "        super().__init__(lm, tokenizer, device=device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Initialize loss function (make it ignore pad tokens). Note the\n",
    "        # use of the reduction=\"none\" keyword argument.\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=tokenizer.pad_token_id, reduction=\"none\")\n",
    "\n",
    "        # Demonstrations for few-shot prompting. When demonstrations are\n",
    "        # used, this variable always ends with \\n\\n. When demonstrations\n",
    "        # are not used, this variable is the empty string.\n",
    "        self._demos = \"\"\n",
    "\n",
    "        # When there is a system prompt, this variable always begins\n",
    "        # with a space, followed by the system prompt. When there is no\n",
    "        # system prompt, this variable is the empty string.\n",
    "        self._system_prompt = \"\"\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.model.name_or_path\n",
    "\n",
    "    @property\n",
    "    def demonstrations(self) -> Optional[str]:\n",
    "        return None if self._demos == \"\" else self._demos[:-2]\n",
    "\n",
    "    def set_demonstrations(self, demos: str):\n",
    "        self._demos = demos + \"\\n\\n\"\n",
    "\n",
    "    def load_demonstrations(self, filename: str):\n",
    "        with open(filename, \"r\") as f:\n",
    "            self.set_demonstrations(f.read())\n",
    "\n",
    "    def clear_demonstrations(self):\n",
    "        self._demos = \"\"\n",
    "\n",
    "    @property\n",
    "    def system_prompt(self) -> Optional[str]:\n",
    "        return None if self._system_prompt == \"\" else self._system_prompt[1:]\n",
    "\n",
    "    def set_system_prompt(self, prompt: str):\n",
    "        self._system_prompt = \" \" + prompt\n",
    "\n",
    "    def clear_system_prompt(self):\n",
    "        self._system_prompt = \"\"\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        \"\"\"\n",
    "        We will not be using this function in this assignment. It is\n",
    "        here because it is an abstract method of the Pipeline class,\n",
    "        which means we have to implement it even if it does nothing.\n",
    "        \"\"\"\n",
    "        return {}, {}, {}\n",
    "\n",
    "    def _get_input_texts(self, batch: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Problem 2c: Implement this function.\n",
    "\n",
    "        This function takes a batch of TruthfulQA questions and forms\n",
    "        the texts that will serve as the input to the LLM. For each\n",
    "        answer choice, the corresponding input text consists of the\n",
    "        prompt, question, and answer choice concatenated together. Dem-\n",
    "        onstrations and system prompts must be included if they are set.\n",
    "        Please make sure that your input texts adhere to the format\n",
    "        illustrated in the problem set.\n",
    "\n",
    "        :param batch: A batch of TruthfulQA questions\n",
    "        :return: The input texts for each answer choice in the batch.\n",
    "            The input texts must appear in order:\n",
    "                text 0 corresponds to answer choice 0 for question 0,\n",
    "                text 1 corresponds to answer choice 1 for question 0,\n",
    "                ...,\n",
    "                text 4 corresponds to answer choice 0 for question 1,\n",
    "                text 5 corresponds to answer choice 1 for question 1,\n",
    "                etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        input_texts = []\n",
    "        for i, one_of_the_questions in enumerate(batch['question']):\n",
    "            input_text = \"\"\n",
    "            answers_for_one_question = batch['choices'][i]\n",
    "            label_for_one_question = batch['label'][i]\n",
    "            for j in range(4):\n",
    "                input_text = \"\"\n",
    "                if self._demos != \"\":\n",
    "                    input_text = f\"{self._demos}\"\n",
    "                if self._system_prompt != \"\":\n",
    "                    answers_for_one_question[j] = self._system_prompt + ' '+ batch['choices'][i][j]\n",
    "                    \n",
    "            # Forming the input text by concatenating prompt, question, and answer choice\n",
    "                input_text += f\"Q:{one_of_the_questions}\\nA:{answers_for_one_question[j]}\"\n",
    "                input_texts.append(input_text)\n",
    "        \n",
    "            \n",
    "        return input_texts\n",
    "        raise NotImplementedError(\"Problem 2c has not been completed yet!\")\n",
    "\n",
    "\n",
    "    def preprocess(self, batch: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Problem 2d: Implement this function.\n",
    "\n",
    "        This function takes a batch of TruthfulQA questions and turns it\n",
    "        into a ðŸ¤— Transformers PyTorch LLM input.\n",
    "\n",
    "        :param batch: A batch of TruthfulQA questions\n",
    "        :return: The LLM input for this batch. The exact contents of the\n",
    "            LLM input depend on what model is being used. For most\n",
    "            models, the input should contain the input texts represented\n",
    "            as a Tensor of vocabulary indices, as well as a Transformer\n",
    "            decoder attention mask represented as a Tensor of 0s and 1s.\n",
    "            These tensors should be stored on the GPU if it is being\n",
    "            used; otherwise, they should be stored on the CPU\n",
    "        \"\"\"\n",
    "        input_texts = self._get_input_texts(batch)\n",
    "        \n",
    "\n",
    "        # Tokenize the input texts and convert to tensor\n",
    "        inputs = self.tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        return inputs\n",
    "        raise NotImplementedError(\"Problem 2d has not been completed yet!\")\n",
    "\n",
    "    def _forward(self, input_: Dict[str, torch.Tensor]) -> \\\n",
    "            Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Problem 2d: Implement this function.\n",
    "\n",
    "        This function takes the output of preprocess and feeds it into\n",
    "        the pipeline's LLM.\n",
    "\n",
    "        :param input_: The output of preprocess, which contains an LLM\n",
    "            input representing a batch of TruthfulQA questions\n",
    "        :return: The logit scores assigned to each next-token prediction\n",
    "            as well as the input_ids tensor from input_\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**input_)\n",
    "            logits = outputs.logits\n",
    "        return {\"input_ids\": input_['input_ids'], \"logits\": logits}\n",
    "        raise NotImplementedError(\"Problem 2d has not been completed yet!\")\n",
    "\n",
    "    def postprocess(self, outputs: Dict[str, torch.Tensor]) -> Output:\n",
    "        \"\"\"\n",
    "        Problem 2d: Implement this function.\n",
    "\n",
    "        This function takes an LLM output, computed by _forward, and for\n",
    "        each question in the batch, identifies the answer choice whose\n",
    "        corresponding input text had the lowest cross-entropy loss.\n",
    "\n",
    "        :param outputs: The output of _forward, which contains the next-\n",
    "            token prediction logits computed by the pipeline's LLM,\n",
    "            along with the vocabulary indices of the input text\n",
    "        :return: The predicted answers (0, 1, 2, or 3) for each question\n",
    "            in the original batch, along with the total cross-entropy\n",
    "            loss incurred by each input text. Make sure your return\n",
    "            value is in the form of an Output named tuple, and make sure\n",
    "            that the losses are formatted as a matrix, where row i cor-\n",
    "            responds to question i and column j corresponds to answer\n",
    "            choice j\n",
    "        \"\"\"\n",
    "        \n",
    "        logits = outputs[\"logits\"]  # Logits for each token prediction\n",
    "        input_ids = outputs[\"input_ids\"]  # Input token indices\n",
    "\n",
    "        total_loss = []\n",
    "        # Calculate cross-entropy loss for each answer choice\n",
    "        batch_size, seq_length, vocab_size = logits.shape\n",
    "        print(batch_size)  #8\n",
    "        print(seq_length)  #66\n",
    "        print(vocab_size)  #50257\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            logit_sample = logits[i][:-1] #0...64\n",
    "            target = input_ids[i][1:]   #1...65\n",
    "            print(logit_sample.shape)\n",
    "            print(target.shape)\n",
    "            loss = self.loss_fn(logit_sample,target)\n",
    "            total_sum = torch.sum(loss)\n",
    "            total_loss.append(total_sum)\n",
    "        \n",
    "        total_loss = torch.tensor(total_loss).reshape(-1,self.num_choices) #2,4\n",
    "            \n",
    "        min_loss_indices = torch.argmin(total_loss, dim = 1)\n",
    "        return Output(loss=total_loss, prediction = min_loss_indices)\n",
    "\n",
    "        \n",
    "        raise NotImplementedError(\"Problem 2d has not been completed yet!\")\n",
    "\n",
    "\n",
    "\n",
    "def run_model(pipeline: MultipleChoicePipeline, dataset: Dataset,\n",
    "              batch_size: int = 10) -> Output:\n",
    "    \"\"\"\n",
    "    Runs a language model on TruthfulQA and returns its predictions and\n",
    "    losses.\n",
    "    \"\"\"\n",
    "    results = [pipeline(dataset[i:i + batch_size])\n",
    "               for i in tqdm(range(0, len(dataset), batch_size))]\n",
    "    return Output(*[np.concatenate(r) for r in zip(*results)])\n",
    "\n",
    "\n",
    "def save_outputs(dataset: Dataset, outputs: Output, filename: str,\n",
    "                 batch_size: int = 50):\n",
    "    \"\"\"\n",
    "    Saves the predictions and losses computed by a language model on\n",
    "    TruthfulQA to a CSV file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as o:\n",
    "        writer = csv.writer(o)\n",
    "        writer.writerow([\"Question\", \"Choice 0\", \"Choice 1\", \"Choice 2\",\n",
    "                         \"Choice 3\", \"Label\", \"Prediction\", \"Loss 0\",\n",
    "                         \"Loss 1\", \"Loss 2\", \"Loss 3\"])\n",
    "\n",
    "        for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "            batch = dataset[i:i + batch_size]\n",
    "\n",
    "            q = batch[\"question\"]\n",
    "            c1, c2, c3, c4 = zip(*batch[\"choices\"])\n",
    "            l_ = batch[\"label\"]\n",
    "            p = outputs.prediction\n",
    "            l1, l2, l3, l4 = outputs.loss.T\n",
    "\n",
    "            for row in zip(q, c1, c2, c3, c4, l_, p, l1, l2, l3, l4):\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "def evaluate_truthfulqa(pipeline: MultipleChoicePipeline, dataset: Dataset,\n",
    "                        batch_size: int = 10):\n",
    "    \"\"\"\n",
    "    Evaluates a pipeline on TruthfulQA.\n",
    "    \"\"\"\n",
    "    global accuracy_metric\n",
    "\n",
    "    # Evaluate the pipeline on TruthfulQA\n",
    "    results = run_model(pipeline, dataset, batch_size=batch_size)\n",
    "    accuracy = accuracy_metric.compute(predictions=results.prediction,\n",
    "                                       references=dataset[\"label\"])\n",
    "\n",
    "    # Save the results as a csv file\n",
    "    model_name = _sanitize(pipeline.name)\n",
    "    no_demos = \"_no_demos\" if pipeline.demonstrations is None else \"\"\n",
    "    system_prompt = \"\" if pipeline.system_prompt is None else \\\n",
    "        \"_\" + _sanitize(pipeline.system_prompt)\n",
    "    fn = f\"results/{model_name}{no_demos}{system_prompt}_predictions_acc\" \\\n",
    "         f\"={accuracy['accuracy']:.3f}.csv\"\n",
    "    save_outputs(dataset, results, fn)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
